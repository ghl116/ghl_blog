# RDD Programming Guide

## 概览

从高层次来看，每个spark应用由一个运行用户main函数的程序组成，并且在集群上执行各种并发操作。spark操作的主要抽象对象是弹性分布式数据集（RDD），是一个跨集群结点可以并发操作的元素集合。RDD可以基于Hadoop文件系统上的文件来创建或者在程序中的Scala集合，然后转化。用户也可以要求spark在持久化内存中的RDD，允许不同的并发操作间高效的重用。最后，RDD可以自动从失败的结点恢复。

spark中的第二个抽像概念是可以用在并行操作中的变量。默认情况下，spark并发运行一个函数，像在不同的结点运行一个任务的合集，它会把函数中变量的副本传递给每个任务。有的时候，变量会在不同的任务之间传递，或者在任务以及驱动程序之间传递。spark支持两种类型的共享变量，广播变量，可以用户来在所有的结点缓存数据，并且可以进行变量的累加，如计数器或者求和。

##Linking with Spark

使用python，pyspark需要在驱动节点和应用节点使用相同的早小版本。

## Initializing Spark

spark程序需要做的第一件事是创建一个sparkContext对象，告诉spark如何访问集群。创建sparkContext第一步是建立sparkconf对象。

实际上，在运行集群时，你不需要硬编码master，而宁愿使用spark-submint来运行程序，接收结果。然而，为了测试，你可以传递local来运行spark.

## Using the Shell

在pyspark shell中，已经有了一个SparkContext，变量名为sc，这会让你自己定义的SparkContext不生效。

- --master
-  --py-files.
-   --packages 
-    --repositories 

也可以在Ipython中运行pyspark，这是一个增强的解析器。pyspark支持ipython1.0.0以上。

## 弹性分布式数据集 (RDDs)

spark是基于RDD来展开的。有两种方式创建rdd：并发处理在驱动程序中的数据集，或者在引入外部系统的数据集，如共享文件系统HDFS，hbase或者任何提供hadoop输入的数据源。

## 并行集合（Parallelized Collections）

在驱动程序中对一个已有的可迭代对象或者集合调用SparkContext’s parallelize 方法可以创建一个并行的集合。集合中的元素被复制到可以并行操作的分布式数据集中。

## 外部数据集

支持文本文件，序列文件以及其他hadoop输入。

### 保存和加载 SequenceFiles

### 保存和加载 其他hadoop的输入输出格式

## RDD操作

RDD支持两种类型的操作：转换，从已有的数据集创建一个新的，行动：在数据集上运行一个计算后向驱动程序返回一个值。例如：map会把每个数据的原素转化为了一个新的RDD结果。另一方面：reduice会聚合RDD中的元素，返回最终的结果。

在spark中所有的转换都是延迟的，他们不会立刻计算结果。相反，他们只会记录转换会应用到哪些数据上。当一个行动需要结果返回给驱动程序时，转换才会被计算。这个设计得到spark运行更高效。例如，我们可以实现创建的数据集在reduce中使用，然后只返回结果，而不是返回大量的map数据结果。

默认情况下，你每次对RDD操作时，每个RDD都会重新计算。然而，你也可以在内存中持久化RDD，这种情况下，spark会在集群中保存数据，下次查询更加快速。也支持把RDD持久化到磁盘或者多个结点上。

## 向spark传递函数

spark api重度依赖在驱动程序中向集群传递函数。有三种推荐方式：

- lambda 表达式，对于一些简单函数可以写成表达式（lambad不支持多行表达式或者表达式没有返回值）
- 在spark本地函数中定义函数，长段代码
- 在模块中的高级别函数

## 理解闭包

spark中最难的事情之一是理解变量和方法的作用域和生命周期当在一个集群中执行代码的时候。RDD操作中在变量的作业域外修改变量是一个非常频繁的困扰。

```
counter = 0
rdd = sc.parallelize(data)

# Wrong: Don't do this!!
def increment_counter(x):
    global counter
    counter += x
rdd.foreach(increment_counter)

print("Counter value: ", counter)
```

## 本地 VS 全局模式

上面的代码执行的结果是undefined，并不会按预计的结果执行。为了执行作业，spark把rdd操作分到task中，每一个会被executor执行。在执行之前，spark会计算task的闭包。变量和方法的闭包一定要对executor可见来执行RDD的计算。这个闭包是可序列号，而且发送到每个executor。

发送到每个executor中的变量是闭包中的变量的副本，因此在foreach函数中的counter被引用时，它已经不是驱动节点的counter. 在驱动节点上的内存中仍然有counter变量，但是不再对executor可见。 executor只可以看到序列化的闭包副本。所以counter的最终值会是0由于所有对counter的操作都是引用了闭包的副本。

在本地模式下，在某些场景下，foreach函数会在驱动器的同一个JVM执行，并且会引用到同一个原始的counter，会导致最终更新它。

应该使用累加器来保证结果的一致。累加器在spark中是用来安全更新变量，当执行被分到集群中的多个结点。在累加器章节会有更说介绍。

一般来说，闭包，构造循环或者定义方法，不应该和全局状态进行更新。spark不会定义或者保证处理的对象是闭包外部的对象。一些代码无法在本地模式下运行，以分布式模式下也不会按预期运行。如果进行全局的聚合，使用累加器是很有必要的。

### 打印RDD元素

另外一个常见的问题是使用rdd.foreach(println) or rdd.map(println)打印RDD的函数。然而，在闭包模式，executors调用stdout的输出现在会输出到executors，而不是驱动节点。如果在驱动节点打印所有的元素，用户可以调用collect方法，先把rdd放到驱动节点，然后调用 rdd.collect().foreach(println)。这会导致驱动节点内存溢出，因为collect会把RDD加载到一台机器，如果你只是需要打印RDD的一部分元素，安全的方法是使用take()：rdd.take(100).foreach(println).

### 使用键值对

spark对RDD的大多数操作包含任何类型的对象，许多特殊操作只支持RDD键值对。比如分布式的shuffle，如按键进行grouping or aggregating。

### 转换

- map
- ……

### 操作


## Shuffle 操作

一些特定的操作会触发spark的shuffle事件。shuffle是spark的重新分布数据的机制以实现跨分区的分组。这会涉及到跨executor和机器的数据复制，所以shuffle是一个复杂有一定代码的操作。



### 性能影响

shuffle操作代价比较高由于会涉及到磁盘I/O，数据序列化和网络I/O。为了组织数据进行shuffle，spark生成了map任务来组织数据，reduce任务来聚合数据。这些命名来源于MapReduce，但是和spark的map和reduce没有直接关系。

在内部，单个任务会把结果保存到内存中直到无法保存。然后，数据会基于目录分区排序，写入到单个文件中。在reduce过程中，任务会读取已排序存存储的内容。

特定的shuffle操作会占用大量的内存因为他们会使用内存数据结构来组织记录。明确的，reduceByKey和aggregateByKey在map端创建结果，ByKey在reduce生成。当数据无法保存在内存中，spark会把他们保存到磁盘，发生磁盘I/O增加垃圾回收。

shuffle也会生成大量的中间文件在磁盘。在spark1.3，这些文件会一直保存直到RDD不会再使用然后进行垃圾回收。这样设计是因为如果数据重新计算，这些文件不需要重新生成。垃圾回收可能在一段时间内发生，如果应用保留了rDD引用，或者GC不频繁。这意味着spark会占用大量的磁盘空间。


## RDD持久化

spark中最重要的能力之一是跨操作持久化或者缓存数据到内存中。当你持久化一个RDD，每个结点会存储它的分区中的计算结果并且在其他操作中重用。它允许未来的操作更快（通常超过10位）。缓存是一个关键的工具做迭代算法以及快速交互使用。

你可以通过persist() or cache()来使RDD持久化。第一次计算后，会被缓存到内存中。spark的缓存是容错的，如果任何一个分区的RDD丢失了，它会自动按之前的转换方式重新计算。


此外，每个持久化的RDD可以使用不同的存储级别来存储，如：磁盘，内存中但是序列化（java），不同结点间复制。


### 选择存储级别

spark的存储级别提供了在内存和CPU效率的不同权衡。我们建议你通过以下方式选择一个：

- 如果RDD可以保存到内存中，那么使用默认的级别MEMORY_ONLY。这是CPU最高效的选项，允许RDD运行的足够快。
- 如果不可以，选择MEMORY_ONLY_SER级别，选择一个最快的序列化库让对象高效的占用空间，而且访问快速。
- 不要把数据放到磁盘中除非计算函数代价很高，或者他们过滤大量的数据。否则，重新计算一个分区会像从磁盘读取一样快。
- 如果你想容错，你可以使用副本存储级别。所有的存储级别都提供完全的容错通过重新计算丢失数据，但是副本级别允许你继续在RDD上运行任务面是需等待重新计算。


### 删除数据

spark自动监控每个节点缓存的使用情况，并且使用LRU方式删除过期旧的分区。如果你想手工删除RDD而不是等待自动移出缓存，可以使用 RDD.unpersist()方法。

## 共享变量

通常来说，当一个函数传给spark在远程节点执行的时候，在函数中会使用变量副本。这些变量被复制到每台机器，而且远程机器对这些变量的更新不会传回驱动程序。读写共享变量在跨任务间是低效的。然而，spark支持两种共享变量：广播变量和计数器。


### 广播变量

广播变量允许程序员把只读变量缓存到每台机器而不是每个任务保存一个变量副本。

spark在一系列的阶段进行操作，被分布式的shuffle操作区分开。spark自动广播每个任务需要使用的常用数据在每个阶段。每个任务运行前，数据的缓存方式叫序列化以及反序列化。这意味着广播变量只有任务跨越多个阶段使用相同的数据或者以反序化的方式缓存数据是非常重要的。


### 计数器


##发布到集群中

参考页面发布到集群application submission guide 

## 启动spark job


## 单元测试

spark支持任何的单元测试框架。简单方式，在你的单元测试中，创建一个sparkcontext设置master URL为local模式，运行操作，调用sparkContext.stop()来结束。确保使用finally来结束上下文，或者使用测试框架的tearDown来结束。






























